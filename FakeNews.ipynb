{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl4zFF+Wcm37UxfgSF7DrG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jatin020408/Multi-Modal-Fake-News-Detector/blob/main/FakeNews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zujj4609c9tx"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version of Fakeddit dataset\n",
        "path = kagglehub.dataset_download(\"vanshikavmittal/fakeddit-dataset\")\n",
        "\n",
        "# Print the path to the dataset files\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "id": "s6W8x01edA8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the downloaded path\n",
        "train_df = pd.read_csv(f'{path}/multimodal_only_samples/multimodal_train.tsv',delimiter='\\t')\n",
        "val_df = pd.read_csv(f'{path}/multimodal_only_samples/multimodal_validate.tsv',delimiter='\\t')\n",
        "test_df = pd.read_csv(f'{path}/multimodal_only_samples/multimodal_test_public.tsv',delimiter='\\t')\n",
        "\n",
        "# Display the first few rows of the training data\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "04B_gIOFdF5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Max token length for BERT input\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Function to tokenize titles\n",
        "def tokenize_titles(df):\n",
        "    return tokenizer(\n",
        "        list(df['clean_title'].values),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=MAX_LEN,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "# Tokenize all three datasets\n",
        "train_encodings = tokenize_titles(train_df)\n",
        "val_encodings = tokenize_titles(val_df)\n",
        "test_encodings = tokenize_titles(test_df)\n",
        "\n",
        "# Check tokenized shapes\n",
        "print(\"Train Input IDs Shape:\", train_encodings['input_ids'].shape)\n",
        "print(\"Validation Input IDs Shape:\", val_encodings['input_ids'].shape)\n"
      ],
      "metadata": {
        "id": "7OqpPWJZAGde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "nest_asyncio.apply()\n",
        "async def fetch_image(session, url):\n",
        "    try:\n",
        "        async with session.get(url, timeout=10) as response:\n",
        "            if response.status == 200:\n",
        "                img_data = await response.read()\n",
        "                img = Image.open(BytesIO(img_data)).convert('RGB')\n",
        "                img = img.resize((224, 224))\n",
        "                img = tf.keras.utils.img_to_array(img)\n",
        "                img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
        "\n",
        "                if img.shape != (224, 224, 3):\n",
        "                    return None\n",
        "                return img\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def process_images_async(urls):\n",
        "    images = []\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for url in tqdm(urls):\n",
        "            img = await fetch_image(session, url)\n",
        "            if img is not None:\n",
        "                images.append(img)\n",
        "            else:\n",
        "                images.append(np.zeros((224, 224, 3)))  # fallback for broken/bad images\n",
        "    return np.array(images, dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "3bmsuWL8dZX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_urls = train_df['image_url'].iloc[:1000].tolist()\n",
        "val_urls = val_df['image_url'].iloc[:500].tolist()\n",
        "test_urls = test_df['image_url'].iloc[:500].tolist()\n",
        "\n",
        "train_images = asyncio.get_event_loop().run_until_complete(process_images_async(train_urls))\n",
        "val_images = asyncio.get_event_loop().run_until_complete(process_images_async(val_urls))\n",
        "test_images = asyncio.get_event_loop().run_until_complete(process_images_async(test_urls))\n",
        "\n",
        "train_images = tf.convert_to_tensor(train_images, dtype=tf.float32)\n",
        "val_images = tf.convert_to_tensor(val_images, dtype=tf.float32)\n",
        "test_images = tf.convert_to_tensor(test_images, dtype=tf.float32)\n",
        "\n",
        "print(\"All images processed\")\n",
        "print(\"Train image shape:\", train_images.shape)\n"
      ],
      "metadata": {
        "id": "D5zFTfwTPzHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save for reuse\n",
        "np.save('/content/train_images.npy', train_images)\n",
        "np.save('/content/val_images.npy', val_images)\n",
        "np.save('/content/test_images.npy', test_images)\n"
      ],
      "metadata": {
        "id": "qD8JScD5NbFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = np.load('/content/train_images.npy')\n",
        "val_images = np.load('/content/val_images.npy')\n",
        "test_images = np.load('/content/test_images.npy')"
      ],
      "metadata": {
        "id": "gxSMDpErUVbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=4):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(embed_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "\n",
        "    def call(self, query, key_value):\n",
        "        # Attention + residual\n",
        "        attn_output = self.mha(query=query, key=key_value, value=key_value)\n",
        "        out1 = self.layernorm(query + attn_output)\n",
        "\n",
        "        # Feedforward + residual\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.layernorm(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "4G2EKgf1PyVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand dims to [batch, 1, dim] so attention sees it as sequence\n",
        "text_embedding_exp = tf.expand_dims(text_embedding, axis=1)     # [batch, 1, 768]\n",
        "effnet_output_exp = tf.expand_dims(effnet_output, axis=1)       # [batch, 1, 1280]\n",
        "\n",
        "# Project EfficientNet features to match BERT dimension (768)\n",
        "image_projected = layers.Dense(768)(effnet_output_exp)          # [batch, 1, 768]\n",
        "\n",
        "# Apply cross-attention\n",
        "cross_attn_layer = CrossAttention(embed_dim=768)\n",
        "attended_text = cross_attn_layer(text_embedding_exp, image_projected)\n",
        "\n",
        "# Flatten back to [batch, 768]\n",
        "attended_text_flat = layers.Flatten()(attended_text)\n"
      ],
      "metadata": {
        "id": "VDWvecF9zX3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nVdYzul_zgDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from transformers import TFBertModel\n",
        "\n",
        "# Load BERT\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "\n",
        "# Inputs\n",
        "text_input = layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "# Lambda wrapper for BERT CLS token\n",
        "def get_bert_cls_embedding(inputs):\n",
        "    input_ids, attention_mask = inputs\n",
        "    outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    return outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "\n",
        "# Explicit shape defined here\n",
        "text_embedding = layers.Lambda(\n",
        "    get_bert_cls_embedding,\n",
        "    output_shape=(768,)\n",
        ")([text_input, attention_mask])\n",
        "\n",
        "# EfficientNetB0 for image features\n",
        "image_input = layers.Input(shape=(224, 224, 3), name=\"image_input\")\n",
        "effnet_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n",
        "effnet_output = effnet_model(image_input)\n",
        "effnet_output = layers.GlobalAveragePooling2D()(effnet_output)\n",
        "\n",
        "# Combine features\n",
        "combined = attended_text_flat  # Already attended to image info\n",
        "x = layers.Dense(512, activation='relu')(combined)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Define and compile model\n",
        "model = models.Model(inputs=[text_input, attention_mask, image_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "zanwEm-Fd1bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trim everything to match train_images\n",
        "train_labels = train_df['2_way_label'].iloc[:1000].values\n",
        "val_labels = val_df['2_way_label'].iloc[:500].values\n",
        "\n",
        "train_input_ids = train_encodings['input_ids'][:1000]\n",
        "train_attention = train_encodings['attention_mask'][:1000]\n",
        "\n",
        "val_input_ids = val_encodings['input_ids'][:500]\n",
        "val_attention = val_encodings['attention_mask'][:500]\n"
      ],
      "metadata": {
        "id": "FQ_rj-yUd35f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,             # stop if val_loss doesn't improve for 2 epochs\n",
        "    restore_best_weights=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "p0tbBVdJRSYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "2mH21yveQIS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    filepath='best_model.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "puObJ_u_R_7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [early_stop, reduce_lr, checkpoint_cb]\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': train_input_ids,\n",
        "        'attention_mask': train_attention,\n",
        "        'image_input': train_images\n",
        "    },\n",
        "    train_labels\n",
        ")).shuffle(1000).batch(16)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': val_input_ids,\n",
        "        'attention_mask': val_attention,\n",
        "        'image_input': val_images\n",
        "    },\n",
        "    val_labels\n",
        ")).batch(16)"
      ],
      "metadata": {
        "id": "6R-OQeNpRGf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ye5BmluRU3d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"final_model.h5\")\n"
      ],
      "metadata": {
        "id": "7wmbI-8sSE1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit text to match image count\n",
        "val_input_ids = val_encodings['input_ids'][:500]\n",
        "val_attention = val_encodings['attention_mask'][:500]\n",
        "val_labels = val_df['2_way_label'].iloc[:500].values\n"
      ],
      "metadata": {
        "id": "XzKfLR4Jd4Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_accuracy = model.evaluate(\n",
        "    [val_input_ids, val_attention, val_images],\n",
        "    val_labels\n",
        ")\n",
        "print(f\" Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "gHbLWJgzd52e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" Validation Loss: {val_loss * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "Hn3eT-_KjiEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g3jgCkBBgLeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predicted classes\n",
        "val_preds = model.predict([\n",
        "    val_encodings['input_ids'][:len(val_images)],\n",
        "    val_encodings['attention_mask'][:len(val_images)],\n",
        "    val_images\n",
        "])\n",
        "val_pred_labels = (val_preds.flatten() > 0.5).astype(int)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(val_labels, val_pred_labels)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix (Validation)')\n",
        "plt.show()\n",
        "\n",
        "# Report\n",
        "print(classification_report(val_labels, val_pred_labels, target_names=['Real', 'Fake']))\n"
      ],
      "metadata": {
        "id": "wsyd9b2cgUI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "def load_and_preprocess_image_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        response.raise_for_status()  # Raise error if not 200 OK\n",
        "\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        img = img.resize((224, 224))\n",
        "        img_array = tf.keras.utils.img_to_array(img)\n",
        "        img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n",
        "        return tf.convert_to_tensor([img_array], dtype=tf.float32)  # Batch dimension\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load image: {url}\")\n",
        "        print(f\"Error: {e}\")\n",
        "        return tf.zeros((1, 224, 224, 3))  # Fallback blank image\n"
      ],
      "metadata": {
        "id": "JpHE-6ueXMQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_idx = 95\n",
        "test_title = test_df['clean_title'].iloc[test_idx]\n",
        "test_url = test_df['image_url'].iloc[test_idx]\n",
        "\n",
        "test_input = tokenizer.encode_plus(\n",
        "    test_title,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    max_length=MAX_LEN,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "test_image = load_and_preprocess_image_from_url(test_url)\n",
        "\n",
        "prediction = model.predict([test_input['input_ids'], test_input['attention_mask'], test_image])\n",
        "label = int(prediction[0][0] > 0.5)\n",
        "\n",
        "print(\"Title:\", test_title)\n",
        "print(\"Image URL:\", test_url)\n",
        "print(\"Prediction:\", \"Fake News \" if label == 0 else \"Real News \")"
      ],
      "metadata": {
        "id": "gv5klTL1Xh54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xk6r9k5WT9Di"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}